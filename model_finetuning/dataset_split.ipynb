{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPR9u2H5wlArZRIsFYAxusK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"_nktaXaAZt0X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670059095194,"user_tz":300,"elapsed":85760,"user":{"displayName":"Peter Zhang","userId":"06410747228434381847"}},"outputId":"a28c103d-f970-4993-c989-f498dc60f3a2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/ECE1786_dataset\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/ECE1786_dataset/"]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V78uB0y98n-d","executionInfo":{"status":"ok","timestamp":1669719199297,"user_tz":300,"elapsed":231,"user":{"displayName":"Peter Zhang","userId":"06410747228434381847"}},"outputId":"edfd0dee-e94c-4295-e475-d1a464f21709"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["final_dataset_suicide.csv\n","final_dataset_suicide_test.csv\n","final_dataset_suicide_test_huggingface.csv\n","final_dataset_suicide_test_openai.csv\n","final_dataset_suicide_train.csv\n","final_dataset_suicide_train_huggingface.csv\n","final_dataset_suicide_train_openai.csv\n","openai_apikey.txt\n"]}]},{"cell_type":"code","source":["!pip install openai\n","!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lgz5_3PC55t7","executionInfo":{"status":"ok","timestamp":1670059186750,"user_tz":300,"elapsed":26950,"user":{"displayName":"Peter Zhang","userId":"06410747228434381847"}},"outputId":"38169c37-6bd0-47df-d84c-41a3a353d95f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting openai\n","  Downloading openai-0.25.0.tar.gz (44 kB)\n","\u001b[K     |████████████████████████████████| 44 kB 2.1 MB/s \n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from openai) (1.21.6)\n","Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.8/dist-packages (from openai) (2.23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from openai) (4.1.1)\n","Requirement already satisfied: openpyxl>=3.0.7 in /usr/local/lib/python3.8/dist-packages (from openai) (3.0.10)\n","Requirement already satisfied: pandas>=1.2.3 in /usr/local/lib/python3.8/dist-packages (from openai) (1.3.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from openai) (4.64.1)\n","Collecting pandas-stubs>=1.1.0.11\n","  Downloading pandas_stubs-1.5.2.221124-py3-none-any.whl (146 kB)\n","\u001b[K     |████████████████████████████████| 146 kB 15.3 MB/s \n","\u001b[?25hRequirement already satisfied: et-xmlfile in /usr/local/lib/python3.8/dist-packages (from openpyxl>=3.0.7->openai) (1.1.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2.3->openai) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2.3->openai) (2022.6)\n","Collecting types-pytz>=2022.1.1\n","  Downloading types_pytz-2022.6.0.1-py3-none-any.whl (4.7 kB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.3->openai) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (1.24.3)\n","Building wheels for collected packages: openai\n","  Building wheel for openai (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for openai: filename=openai-0.25.0-py3-none-any.whl size=55880 sha256=1aa11cbf9d42ddb3be400239515c75484ef560bf7294f1333145ce285bce2cc2\n","  Stored in directory: /root/.cache/pip/wheels/4b/92/33/6f57c7aae0b16875267999a50570e81f15eecec577ebe05a2e\n","Successfully built openai\n","Installing collected packages: types-pytz, pandas-stubs, openai\n","Successfully installed openai-0.25.0 pandas-stubs-1.5.2.221124 types-pytz-2022.6.0.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n","\u001b[K     |████████████████████████████████| 5.8 MB 6.6 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 53.6 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 38.0 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"]}]},{"cell_type":"code","source":["import openai\n","import pandas as pd\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"QiUgk0L6f-IS","executionInfo":{"status":"ok","timestamp":1670059191773,"user_tz":300,"elapsed":2756,"user":{"displayName":"Peter Zhang","userId":"06410747228434381847"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv(\"./final_dataset_suicide.csv\")\n","X, y = df['text'], df['class']"],"metadata":{"id":"O_Fm-MdHmilz","executionInfo":{"status":"ok","timestamp":1670060941379,"user_tz":300,"elapsed":7418,"user":{"displayName":"Peter Zhang","userId":"06410747228434381847"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=114514, stratify=y)"],"metadata":{"id":"hQt76WIlnTFE","executionInfo":{"status":"ok","timestamp":1670060950976,"user_tz":300,"elapsed":325,"user":{"displayName":"Peter Zhang","userId":"06410747228434381847"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["print(\"===== train dataset =====\")\n","print(\"# of rows: \", y_train.count())\n","print(\"# of suicide labels: \", y_train[y_train==\"suicide\"].count())\n","print(\"# of non-suicide labels: \", y_train[y_train==\"non-suicide\"].count())\n","print(\"===== validation dataset =====\")\n","print(\"# of rows: \", y_test.count())\n","print(\"# of suicide labels: \", y_test[y_test==\"suicide\"].count())\n","print(\"# of non-suicide labels: \", y_test[y_test==\"non-suicide\"].count())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ArPaagdSoYIE","executionInfo":{"status":"ok","timestamp":1670060952036,"user_tz":300,"elapsed":149,"user":{"displayName":"Peter Zhang","userId":"06410747228434381847"}},"outputId":"2f205d37-16e3-4f7b-f667-db2af7302603"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["===== train dataset =====\n","# of rows:  176000\n","# of suicide labels:  88000\n","# of non-suicide labels:  88000\n","===== validation dataset =====\n","# of rows:  44000\n","# of suicide labels:  22000\n","# of non-suicide labels:  22000\n"]}]},{"cell_type":"code","source":["train_df = pd.DataFrame({'text': X_train, 'class': y_train}).reset_index()\n","test_df = pd.DataFrame({'text': X_test, 'class': y_test}).reset_index()"],"metadata":{"id":"WTKFOmLbo8rx","executionInfo":{"status":"ok","timestamp":1670060955187,"user_tz":300,"elapsed":147,"user":{"displayName":"Peter Zhang","userId":"06410747228434381847"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["train_df.to_csv(\"./final_dataset_suicide_train.csv\", index=False, index_label=None)\n","test_df.to_csv(\"./final_dataset_suicide_test.csv\", index=False, index_label=None)"],"metadata":{"id":"qhbAZKghrqgu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## openai dataset"],"metadata":{"id":"dpgCkZCx8MLu"}},{"cell_type":"code","source":["train_df.rename(columns={\"text\": \"prompt\", \"class\": \"completion\"}, inplace=True)\n","test_df.rename(columns={\"text\": \"prompt\", \"class\": \"completion\"}, inplace=True)"],"metadata":{"id":"zaQI385465lF","executionInfo":{"status":"ok","timestamp":1670060996939,"user_tz":300,"elapsed":122,"user":{"displayName":"Peter Zhang","userId":"06410747228434381847"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["### 1/10 openai"],"metadata":{"id":"qEJN-zA1audI"}},{"cell_type":"code","source":["X_train_10, _, y_train_10, _ = train_test_split(train_df.prompt, train_df.completion, test_size=0.9, random_state=1919819, stratify=train_df.completion)"],"metadata":{"id":"hicLb0dcaxYp","executionInfo":{"status":"ok","timestamp":1670064226531,"user_tz":300,"elapsed":301,"user":{"displayName":"Peter Zhang","userId":"06410747228434381847"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["_, X_test_10, _, y_test_10 = train_test_split(test_df.prompt, test_df.completion, test_size=0.1, random_state=1919819, stratify=test_df.completion)"],"metadata":{"id":"76z9e2bqhClj","executionInfo":{"status":"ok","timestamp":1670064227517,"user_tz":300,"elapsed":116,"user":{"displayName":"Peter Zhang","userId":"06410747228434381847"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["train_10_df = pd.DataFrame({'prompt': X_train_10, 'completion': y_train_10}).reset_index()\n","test_10_df = pd.DataFrame({'prompt': X_test_10, 'completion': y_test_10}).reset_index()"],"metadata":{"id":"m9AG5qLxgrU-","executionInfo":{"status":"ok","timestamp":1670064232677,"user_tz":300,"elapsed":132,"user":{"displayName":"Peter Zhang","userId":"06410747228434381847"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["train_10_df.to_csv(\"final_dataset_suicide_train_10_openai.csv\", index=False, index_label=None)\n","test_10_df.to_csv(\"final_dataset_suicide_test_10_openai.csv\", index=False, index_label=None)"],"metadata":{"id":"Qo63qVf5lf0_","executionInfo":{"status":"ok","timestamp":1670064299847,"user_tz":300,"elapsed":748,"user":{"displayName":"Peter Zhang","userId":"06410747228434381847"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["!openai tools fine_tunes.prepare_data -f \"final_dataset_suicide_train_10_openai.csv\"\n","!openai tools fine_tunes.prepare_data -f \"final_dataset_suicide_test_10_openai.csv\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HJiYMaTvlvtM","executionInfo":{"status":"ok","timestamp":1670064392828,"user_tz":300,"elapsed":40949,"user":{"displayName":"Peter Zhang","userId":"06410747228434381847"}},"outputId":"834f3af5-60d1-49d3-f938-38b913693b2b"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Analyzing...\n","\n","- Based on your file extension, your file is formatted as a CSV file\n","- Your file contains 17600 prompt-completion pairs\n","- The input file should contain exactly two columns/keys per row. Additional columns/keys present are: ['index']\n","  WARNING: Some of the additional columns/keys contain `index` in their name. These will be ignored, and the column/key `index` will be used instead. This could also result from a duplicate column/key in the provided file.\n","- Based on your data it seems like you're trying to fine-tune a model for classification\n","- For classification, we recommend you try one of the faster and cheaper models, such as `ada`\n","- For classification, you can estimate the expected model performance by keeping a held out dataset, which is not used for training\n","- There are 35 examples that are very long. These are rows: [627, 1269, 1597, 1602, 2804, 3250, 3428, 4176, 4394, 4780, 5528, 5614, 5870, 5956, 7202, 7753, 8464, 8600, 9545, 9773, 10050, 10625, 11503, 11711, 12472, 12494, 12921, 13794, 14122, 15537, 16597, 16651, 16714, 16911, 16984]\n","For conditional generation, and for classification the examples shouldn't be longer than 2048 tokens.\n","- Your data does not contain a common separator at the end of your prompts. Having a separator string appended to the end of the prompt makes it clearer to the fine-tuned model where the completion should begin. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples. If you intend to do open-ended generation, then you should leave the prompts empty\n","- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n","\n","Based on the analysis we will perform the following actions:\n","- [Necessary] Your format `CSV` will be converted to `JSONL`\n","- [Necessary] Remove additional columns/keys: ['index']\n","- [Recommended] Remove 35 long examples [Y/n]: Y\n","- [Recommended] Add a suffix separator `\\n\\n###\\n\\n` to all prompts [Y/n]: Y\n","- [Recommended] Add a whitespace character to the beginning of the completion [Y/n]: Y\n","- [Recommended] Would you like to split into training and validation set? [Y/n]: n\n","\n","\n","Your data will be written to a new JSONL file. Proceed [Y/n]: Y\n","\n","Wrote modified file to `final_dataset_suicide_train_10_openai_prepared.jsonl`\n","Feel free to take a look!\n","\n","Now use that file when fine-tuning:\n","> openai api fine_tunes.create -t \"final_dataset_suicide_train_10_openai_prepared.jsonl\"\n","\n","After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string `\\n\\n###\\n\\n` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\"suicide\"]` so that the generated texts ends at the expected place.\n","Once your model starts training, it'll approximately take 7.06 hours to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n","Analyzing...\n","\n","- Based on your file extension, your file is formatted as a CSV file\n","- Your file contains 4400 prompt-completion pairs\n","- The input file should contain exactly two columns/keys per row. Additional columns/keys present are: ['index']\n","  WARNING: Some of the additional columns/keys contain `index` in their name. These will be ignored, and the column/key `index` will be used instead. This could also result from a duplicate column/key in the provided file.\n","- Based on your data it seems like you're trying to fine-tune a model for classification\n","- For classification, we recommend you try one of the faster and cheaper models, such as `ada`\n","- For classification, you can estimate the expected model performance by keeping a held out dataset, which is not used for training\n","- There are 8 examples that are very long. These are rows: [149, 1352, 1436, 1467, 2281, 2362, 2711, 4256]\n","For conditional generation, and for classification the examples shouldn't be longer than 2048 tokens.\n","- Your data does not contain a common separator at the end of your prompts. Having a separator string appended to the end of the prompt makes it clearer to the fine-tuned model where the completion should begin. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples. If you intend to do open-ended generation, then you should leave the prompts empty\n","- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n","\n","Based on the analysis we will perform the following actions:\n","- [Necessary] Your format `CSV` will be converted to `JSONL`\n","- [Necessary] Remove additional columns/keys: ['index']\n","- [Recommended] Remove 8 long examples [Y/n]: Y\n","- [Recommended] Add a suffix separator `\\n\\n###\\n\\n` to all prompts [Y/n]: Y\n","- [Recommended] Add a whitespace character to the beginning of the completion [Y/n]: Y\n","- [Recommended] Would you like to split into training and validation set? [Y/n]: n\n","\n","\n","Your data will be written to a new JSONL file. Proceed [Y/n]: Y\n","\n","Wrote modified file to `final_dataset_suicide_test_10_openai_prepared.jsonl`\n","Feel free to take a look!\n","\n","Now use that file when fine-tuning:\n","> openai api fine_tunes.create -t \"final_dataset_suicide_test_10_openai_prepared.jsonl\"\n","\n","After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string `\\n\\n###\\n\\n` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\"suicide\"]` so that the generated texts ends at the expected place.\n","Once your model starts training, it'll approximately take 1.8 hours to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"]}]},{"cell_type":"markdown","source":["### openai"],"metadata":{"id":"QjPsBEEPZM6d"}},{"cell_type":"code","source":["train_df.to_csv(\"final_dataset_suicide_train_openai.csv\", index=False, index_label=None)\n","test_df.to_csv(\"final_dataset_suicide_test_openai.csv\", index=False, index_label=None)"],"metadata":{"id":"RBouhggu8KUm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!openai tools fine_tunes.prepare_data -f \"final_dataset_suicide_train_openai.csv\"\n","!openai tools fine_tunes.prepare_data -f \"final_dataset_suicide_test_openai.csv\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"igmjajFJ50Vb","outputId":"25bc3471-b907-4a60-8b9d-a3fe3ae7ed7c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Analyzing...\n","\n","- Based on your file extension, your file is formatted as a CSV file\n","- Your file contains 176000 prompt-completion pairs\n","- The input file should contain exactly two columns/keys per row. Additional columns/keys present are: ['index']\n","  WARNING: Some of the additional columns/keys contain `index` in their name. These will be ignored, and the column/key `index` will be used instead. This could also result from a duplicate column/key in the provided file.\n","- Based on your data it seems like you're trying to fine-tune a model for classification\n","- For classification, we recommend you try one of the faster and cheaper models, such as `ada`\n","- For classification, you can estimate the expected model performance by keeping a held out dataset, which is not used for training\n","- There are 287 examples that are very long. These are rows: [399, 495, 848, 1677, 1933, 2222, 2490, 2599, 2764, 2951, 4457, 4538, 5709, 5883, 5898, 6048, 6080, 7029, 8926, 9107, 9444, 9485, 11028, 11893, 12414, 14365, 15759, 16923, 17183, 17349, 17663, 18915, 18976, 20161, 20652, 22751, 22794, 25060, 25714, 27099, 27143, 28313, 29061, 29315, 29591, 31084, 31563, 31699, 32348, 32922, 33202, 33992, 34757, 35019, 35222, 35578, 35826, 36646, 37224, 37524, 37947, 38256, 39903, 40980, 41153, 41707, 42185, 42425, 42435, 43247, 43977, 44178, 44505, 44913, 45026, 45733, 47375, 47742, 48318, 48660, 49631, 50133, 51648, 52099, 53918, 54829, 55395, 55425, 55642, 55709, 56966, 58673, 59119, 59486, 59655, 59835, 59959, 60250, 60516, 60953, 61577, 61730, 62970, 63273, 64005, 64553, 65470, 66100, 66267, 66598, 67066, 67562, 68523, 68933, 69362, 70566, 70902, 71208, 72604, 72682, 76750, 76755, 77139, 77158, 77620, 77726, 77892, 78924, 79125, 79394, 79622, 80928, 81151, 81156, 81979, 83579, 84462, 85057, 85352, 87953, 88909, 92328, 92703, 94461, 94660, 96923, 98013, 98040, 98441, 98992, 100162, 100314, 100782, 101264, 101895, 102422, 102577, 103220, 104066, 104852, 105368, 105449, 106898, 107215, 107896, 107923, 108668, 109053, 109072, 110469, 110548, 110555, 110860, 110914, 111315, 111481, 111548, 111924, 112469, 112592, 112728, 112751, 114079, 114598, 114834, 115539, 116027, 117103, 117184, 117418, 117426, 117523, 118031, 118310, 118392, 118495, 118800, 118931, 118963, 119250, 119302, 119681, 120416, 123031, 123186, 123463, 124906, 124924, 125341, 126450, 128194, 129543, 129651, 131805, 132193, 133294, 133487, 133691, 133731, 133884, 134161, 134239, 134797, 136116, 136181, 136459, 137559, 138011, 138401, 138825, 138902, 142591, 142768, 143314, 143509, 144235, 145068, 145114, 145376, 146474, 146553, 147073, 147534, 148147, 149300, 149450, 150969, 151082, 151794, 152108, 152961, 154961, 156054, 156991, 157398, 158641, 158945, 159604, 159707, 160291, 160976, 161065, 161431, 161894, 162372, 163945, 163975, 164008, 165268, 165885, 166043, 167421, 167778, 167969, 168190, 168382, 169343, 169346, 169611, 172018, 172082, 172395, 172553, 172780, 173718, 173932, 174022]\n","For conditional generation, and for classification the examples shouldn't be longer than 2048 tokens.\n","- Your data does not contain a common separator at the end of your prompts. Having a separator string appended to the end of the prompt makes it clearer to the fine-tuned model where the completion should begin. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples. If you intend to do open-ended generation, then you should leave the prompts empty\n","- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n","\n","Based on the analysis we will perform the following actions:\n","- [Necessary] Your format `CSV` will be converted to `JSONL`\n","- [Necessary] Remove additional columns/keys: ['index']\n","- [Recommended] Remove 287 long examples [Y/n]: Y\n","- [Recommended] Add a suffix separator `\\n\\n###\\n\\n` to all prompts [Y/n]: Y\n","- [Recommended] Add a whitespace character to the beginning of the completion [Y/n]: Y\n","- [Recommended] Would you like to split into training and validation set? [Y/n]: n\n","\n","\n","Your data will be written to a new JSONL file. Proceed [Y/n]: Y\n","\n","Wrote modified file to `final_dataset_suicide_train_openai_prepared.jsonl`\n","Feel free to take a look!\n","\n","Now use that file when fine-tuning:\n","> openai api fine_tunes.create -t \"final_dataset_suicide_train_openai_prepared.jsonl\"\n","\n","After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string `\\n\\n###\\n\\n` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\"suicide\"]` so that the generated texts ends at the expected place.\n","Once your model starts training, it'll approximately take 2.93 days to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n","Analyzing...\n","\n","- Based on your file extension, your file is formatted as a CSV file\n","- Your file contains 44000 prompt-completion pairs\n","- The input file should contain exactly two columns/keys per row. Additional columns/keys present are: ['index']\n","  WARNING: Some of the additional columns/keys contain `index` in their name. These will be ignored, and the column/key `index` will be used instead. This could also result from a duplicate column/key in the provided file.\n","- Based on your data it seems like you're trying to fine-tune a model for classification\n","- For classification, we recommend you try one of the faster and cheaper models, such as `ada`\n","- For classification, you can estimate the expected model performance by keeping a held out dataset, which is not used for training\n","- There are 67 examples that are very long. These are rows: [20, 366, 1018, 1168, 2684, 2790, 3605, 5771, 7138, 9639, 10233, 10530, 10721, 10736, 11683, 12082, 12333, 12483, 13920, 15542, 15611, 16683, 16791, 16956, 18034, 18416, 19145, 19240, 19616, 19791, 19917, 20307, 21180, 22447, 22554, 22610, 23380, 23610, 23651, 24131, 25232, 25611, 27027, 27940, 29558, 30112, 30540, 31503, 32925, 34123, 34736, 34860, 34873, 36802, 37557, 37618, 38040, 38479, 38934, 39256, 39409, 41262, 41979, 42308, 42533, 42827, 43271]\n","For conditional generation, and for classification the examples shouldn't be longer than 2048 tokens.\n","- Your data does not contain a common separator at the end of your prompts. Having a separator string appended to the end of the prompt makes it clearer to the fine-tuned model where the completion should begin. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples. If you intend to do open-ended generation, then you should leave the prompts empty\n","- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n","\n","Based on the analysis we will perform the following actions:\n","- [Necessary] Your format `CSV` will be converted to `JSONL`\n","- [Necessary] Remove additional columns/keys: ['index']\n","- [Recommended] Remove 67 long examples [Y/n]: Y\n","- [Recommended] Add a suffix separator `\\n\\n###\\n\\n` to all prompts [Y/n]: Y\n","- [Recommended] Add a whitespace character to the beginning of the completion [Y/n]: Y\n","- [Recommended] Would you like to split into training and validation set? [Y/n]: "]}]},{"cell_type":"markdown","source":["### hugging face"],"metadata":{"id":"ZbTNbTrQapIP"}},{"cell_type":"code","source":["train_df.drop([399, 495, 848, 1677, 1933, 2222, 2490, 2599, 2764, 2951, 4457, 4538, 5709, 5883, 5898, 6048, 6080, 7029, 8926, 9107, 9444, 9485, 11028, 11893, 12414, 14365, 15759, 16923, 17183, 17349, 17663, 18915, 18976, 20161, 20652, 22751, 22794, 25060, 25714, 27099, 27143, 28313, 29061, 29315, 29591, 31084, 31563, 31699, 32348, 32922, 33202, 33992, 34757, 35019, 35222, 35578, 35826, 36646, 37224, 37524, 37947, 38256, 39903, 40980, 41153, 41707, 42185, 42425, 42435, 43247, 43977, 44178, 44505, 44913, 45026, 45733, 47375, 47742, 48318, 48660, 49631, 50133, 51648, 52099, 53918, 54829, 55395, 55425, 55642, 55709, 56966, 58673, 59119, 59486, 59655, 59835, 59959, 60250, 60516, 60953, 61577, 61730, 62970, 63273, 64005, 64553, 65470, 66100, 66267, 66598, 67066, 67562, 68523, 68933, 69362, 70566, 70902, 71208, 72604, 72682, 76750, 76755, 77139, 77158, 77620, 77726, 77892, 78924, 79125, 79394, 79622, 80928, 81151, 81156, 81979, 83579, 84462, 85057, 85352, 87953, 88909, 92328, 92703, 94461, 94660, 96923, 98013, 98040, 98441, 98992, 100162, 100314, 100782, 101264, 101895, 102422, 102577, 103220, 104066, 104852, 105368, 105449, 106898, 107215, 107896, 107923, 108668, 109053, 109072, 110469, 110548, 110555, 110860, 110914, 111315, 111481, 111548, 111924, 112469, 112592, 112728, 112751, 114079, 114598, 114834, 115539, 116027, 117103, 117184, 117418, 117426, 117523, 118031, 118310, 118392, 118495, 118800, 118931, 118963, 119250, 119302, 119681, 120416, 123031, 123186, 123463, 124906, 124924, 125341, 126450, 128194, 129543, 129651, 131805, 132193, 133294, 133487, 133691, 133731, 133884, 134161, 134239, 134797, 136116, 136181, 136459, 137559, 138011, 138401, 138825, 138902, 142591, 142768, 143314, 143509, 144235, 145068, 145114, 145376, 146474, 146553, 147073, 147534, 148147, 149300, 149450, 150969, 151082, 151794, 152108, 152961, 154961, 156054, 156991, 157398, 158641, 158945, 159604, 159707, 160291, 160976, 161065, 161431, 161894, 162372, 163945, 163975, 164008, 165268, 165885, 166043, 167421, 167778, 167969, 168190, 168382, 169343, 169346, 169611, 172018, 172082, 172395, 172553, 172780, 173718, 173932, 174022], axis=0, inplace=True)"],"metadata":{"id":"yfBtO49z2TJ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df.drop([20, 366, 1018, 1168, 2684, 2790, 3605, 5771, 7138, 9639, 10233, 10530, 10721, 10736, 11683, 12082, 12333, 12483, 13920, 15542, 15611, 16683, 16791, 16956, 18034, 18416, 19145, 19240, 19616, 19791, 19917, 20307, 21180, 22447, 22554, 22610, 23380, 23610, 23651, 24131, 25232, 25611, 27027, 27940, 29558, 30112, 30540, 31503, 32925, 34123, 34736, 34860, 34873, 36802, 37557, 37618, 38040, 38479, 38934, 39256, 39409, 41262, 41979, 42308, 42533, 42827, 43271], axis=0, inplace=True)"],"metadata":{"id":"jlEF_rQd6-O-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df.to_csv(\"final_dataset_suicide_train_huggingface.csv\", index=False, index_label=None)\n","test_df.to_csv(\"final_dataset_suicide_test_huggingface.csv\", index=False, index_label=None)"],"metadata":{"id":"p_ZhC1I-9Wrc"},"execution_count":null,"outputs":[]}]}